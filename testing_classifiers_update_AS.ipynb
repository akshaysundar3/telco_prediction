{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Akshay: elastic net, SVM\n",
    "  \n",
    "Jørgen: LDA, random forests (decision trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"telco_train.csv\", index_col=0)\n",
    "df_test = pd.read_csv(\"telco_test.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection and Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>senior</th>\n",
       "      <th>partner</th>\n",
       "      <th>dependents</th>\n",
       "      <th>tenure</th>\n",
       "      <th>phone</th>\n",
       "      <th>multiple_lines</th>\n",
       "      <th>online_security</th>\n",
       "      <th>online_backup</th>\n",
       "      <th>device_protection</th>\n",
       "      <th>...</th>\n",
       "      <th>monthly_charges</th>\n",
       "      <th>total_charges</th>\n",
       "      <th>internet_dsl</th>\n",
       "      <th>internet_fiber</th>\n",
       "      <th>contract_1_yr</th>\n",
       "      <th>contract_2_yr</th>\n",
       "      <th>payment_credit_card</th>\n",
       "      <th>payment_electronic_check</th>\n",
       "      <th>payment_mailed_check</th>\n",
       "      <th>churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2142</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>64.85</td>\n",
       "      <td>1336.80</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1623</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>97.20</td>\n",
       "      <td>5129.45</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6074</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>23.45</td>\n",
       "      <td>23.45</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1362</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>70.20</td>\n",
       "      <td>237.95</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6754</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>61.90</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1212</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>69.55</td>\n",
       "      <td>521.35</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2722</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>18.95</td>\n",
       "      <td>613.95</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4006</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>72</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>19.80</td>\n",
       "      <td>1414.65</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6791</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>39.65</td>\n",
       "      <td>733.35</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5466</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>19.95</td>\n",
       "      <td>219.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      gender  senior  partner  dependents  tenure  phone  multiple_lines  \\\n",
       "2142       0       0        0           1      21      1               0   \n",
       "1623       0       0        0           0      54      1               1   \n",
       "6074       1       0        1           0       1      0               0   \n",
       "1362       1       0        0           0       4      1               0   \n",
       "6754       1       0        0           1       0      1               1   \n",
       "1212       1       0        0           0       7      1               0   \n",
       "2722       1       0        1           1      32      1               0   \n",
       "4006       1       0        1           1      72      1               0   \n",
       "6791       1       0        0           0      19      0               0   \n",
       "5466       1       0        0           1      10      1               0   \n",
       "\n",
       "      online_security  online_backup  device_protection  ...  monthly_charges  \\\n",
       "2142                1              0                  1  ...            64.85   \n",
       "1623                0              1                  0  ...            97.20   \n",
       "6074                0              0                  0  ...            23.45   \n",
       "1362                0              0                  0  ...            70.20   \n",
       "6754                1              1                  0  ...            61.90   \n",
       "1212                0              0                  0  ...            69.55   \n",
       "2722                0              0                  0  ...            18.95   \n",
       "4006                0              0                  0  ...            19.80   \n",
       "6791                0              0                  1  ...            39.65   \n",
       "5466                0              0                  0  ...            19.95   \n",
       "\n",
       "      total_charges  internet_dsl  internet_fiber  contract_1_yr  \\\n",
       "2142        1336.80             1               0              1   \n",
       "1623        5129.45             0               1              0   \n",
       "6074          23.45             1               0              0   \n",
       "1362         237.95             0               1              0   \n",
       "6754           0.00             1               0              0   \n",
       "1212         521.35             0               1              0   \n",
       "2722         613.95             0               0              0   \n",
       "4006        1414.65             0               0              0   \n",
       "6791         733.35             1               0              0   \n",
       "5466         219.50             0               0              1   \n",
       "\n",
       "      contract_2_yr  payment_credit_card  payment_electronic_check  \\\n",
       "2142              0                    0                         0   \n",
       "1623              1                    0                         0   \n",
       "6074              0                    0                         1   \n",
       "1362              0                    0                         1   \n",
       "6754              1                    0                         0   \n",
       "1212              0                    0                         1   \n",
       "2722              1                    0                         0   \n",
       "4006              1                    1                         0   \n",
       "6791              0                    0                         1   \n",
       "5466              0                    0                         0   \n",
       "\n",
       "      payment_mailed_check  churn  \n",
       "2142                     1      0  \n",
       "1623                     0      0  \n",
       "6074                     0      1  \n",
       "1362                     0      1  \n",
       "6754                     0      0  \n",
       "1212                     0      0  \n",
       "2722                     1      0  \n",
       "4006                     0      0  \n",
       "6791                     0      1  \n",
       "5466                     0      0  \n",
       "\n",
       "[10 rows x 24 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1502\n"
     ]
    }
   ],
   "source": [
    "# Adding feature for average charges by tenure\n",
    "\n",
    "df[\"average_charges\"] = df.apply(lambda x: 0.0 if int(x[\"tenure\"])==0 else x[\"total_charges\"]/float(x[\"tenure\"]), axis=1)\n",
    "df_test[\"average_charges\"] = df_test.apply(lambda x: 0.0 if int(x[\"tenure\"])==0 else x[\"total_charges\"]/float(x[\"tenure\"]), axis=1)\n",
    "\n",
    "df[\"single_parent\"] = df.apply(lambda x: 1 if (int(x['partner'])==0 and int(x['dependents'])!=0) else 0, axis=1)\n",
    "df_test[\"single_parent\"] = df_test.apply(lambda x: 1 if (int(x['partner'])==0 and int(x['dependents'])!=0) else 0, axis=1)\n",
    "\n",
    "df[\"lonely_senior\"] = df.apply(lambda x: 1 if (int(x['partner'])==0 and int(x['dependents'])==0 and int(x['senior'])==1) else 0, axis=1)\n",
    "df_test[\"lonely_senior\"] = df_test.apply(lambda x: 1 if (int(x['partner'])==0 and int(x['dependents'])==0 and int(x['senior'])==1) else 0, axis=1)\n",
    "\n",
    "df[\"repeat_renewer\"] = df.apply(lambda x: 1 if (int(x['tenure'])>12 and int(x['contract_1_yr'])==0 and int(x['contract_2_yr'])==0) else 0, axis=1)\n",
    "df_test[\"repeat_renewer\"] = df_test.apply(lambda x: 1 if (int(x['tenure'])>12 and int(x['contract_1_yr'])==0 and int(x['contract_2_yr'])==0) else 0, axis=1)\n",
    "\n",
    "print(np.sum(df[\"repeat_renewer\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop([\"churn\"], axis=1)\n",
    "#y = pd.reset_index(df.churn)\n",
    "y = df.churn.reset_index().churn\n",
    "\n",
    "X_test = df_test.drop([\"churn\"], axis=1)\n",
    "y_test = df_test.churn.reset_index().churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.astype(np.float64)\n",
    "X_test = X_test.astype(np.float64)\n",
    "Xu = X.values\n",
    "Xu_test = X_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=10, random_state=None, shuffle=False)\n"
     ]
    }
   ],
   "source": [
    "#Split for Cross Validation\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=10)\n",
    "kf.get_n_splits(X)\n",
    "print(kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score, log_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to Running models, we can set a baseline by calculating the null accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7344692935747249"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-np.mean(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic Regression on training set: 0.80\n",
      "Accuracy of Logistic Regression on test set: 0.81\n",
      "0.7994290339241401\n",
      "0.5712763973228642\n",
      "0.7059542035844095\n"
     ]
    }
   ],
   "source": [
    "# Simple Logistic Regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "\n",
    "logreg_basic = LogisticRegression(solver=\"lbfgs\", random_state=0)\n",
    "logreg_basic.fit(X, y)\n",
    "print(\"Accuracy of Logistic Regression on training set: {:.2f}\".format(logreg_basic.score(X, y)))\n",
    "print(\"Accuracy of Logistic Regression on test set: {:.2f}\".format(logreg_basic.score(X_test, y_test)))\n",
    "\n",
    "# Generalized Error\n",
    "\n",
    "gen_error_lrb = list()\n",
    "f1_lrb = list()\n",
    "ra_lrb = list()\n",
    "ll_lrb = list()\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    logreg_basic_gen = LogisticRegression(solver=\"lbfgs\", random_state=0)\n",
    "    logreg_basic_gen.fit(X[train_index], y[train_index])\n",
    "    gen_error_lrb.append(logreg_basic_gen.score(X[test_index], y[test_index]))\n",
    "    y_pred = logreg_basic_gen.predict(X[test_index])\n",
    "    f1_lrb.append(f1_score(y[test_index],y_pred))\n",
    "    ra_lrb.append(roc_auc_score(y[test_index],y_pred))\n",
    "    ll_lrb.append(log_loss(y[test_index],y_pred))\n",
    "    \n",
    "print(np.mean(gen_error_lrb))\n",
    "print(np.mean(f1_lrb))\n",
    "print(np.mean(ra_lrb))\n",
    "#print(np.mean(ll_lrb))\n",
    "#print(log_loss(y,logreg_basic.predict(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8022719204827831"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing RFE\n",
    "\n",
    "selector = RFE(logreg_basic, step=1)\n",
    "selector.fit(X,y)\n",
    "\n",
    "selector.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://gist.github.com/rspeare/77061e6e317896be29c6de9a85db301d\n",
    "\n",
    "from sklearn import linear_model\n",
    "import numpy as np\n",
    "import scipy.stats as stat\n",
    "\n",
    "class LogisticReg:\n",
    "    \"\"\"\n",
    "    Wrapper Class for Logistic Regression which has the usual sklearn instance \n",
    "    in an attribute self.model, and pvalues, z scores and estimated \n",
    "    errors for each coefficient in \n",
    "    \n",
    "    self.z_scores\n",
    "    self.p_values\n",
    "    self.sigma_estimates\n",
    "    \n",
    "    as well as the negative hessian of the log Likelihood (Fisher information)\n",
    "    \n",
    "    self.F_ij\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,*args,**kwargs):#,**kwargs):\n",
    "        self.model = linear_model.LogisticRegression(*args,**kwargs)#,**args)\n",
    "\n",
    "    def fit(self,X,y):\n",
    "        self.model.fit(X,y)\n",
    "        #### Get p-values for the fitted model ####\n",
    "        denom = (2.0*(1.0+np.cosh(self.model.decision_function(X))))\n",
    "        denom = np.tile(denom,(X.shape[1],1)).T\n",
    "        F_ij = np.dot((X/denom).T,X) ## Fisher Information Matrix\n",
    "        Cramer_Rao = np.linalg.inv(F_ij) ## Inverse Information Matrix\n",
    "        sigma_estimates = np.sqrt(np.diagonal(Cramer_Rao))\n",
    "        z_scores = self.model.coef_[0]/sigma_estimates # z-score for eaach model coefficient\n",
    "        p_values = [stat.norm.sf(abs(x))*2 for x in z_scores] ### two tailed test for p-values\n",
    "        \n",
    "        self.z_scores = z_scores\n",
    "        self.p_values = p_values\n",
    "        self.sigma_estimates = sigma_estimates\n",
    "        self.F_ij = F_ij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr1 = LogisticReg(solver=\"lbfgs\", random_state=0)\n",
    "# lr1.fit(X,y)\n",
    "# z = lr1.z_scores\n",
    "# p = lr1.p_values\n",
    "# zipped = list(zip(list(features), z, p))\n",
    "# lr1_summary = pd.DataFrame(zipped)\n",
    "# lr1_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Ridge Logistic Regression on training set: 0.80\n",
      "Accuracy of Ridge Logistic Regression on test set: 0.81\n",
      "0.08\n",
      "0.8012030283561972\n",
      "0.5732844092701364\n",
      "0.7070690956857325\n"
     ]
    }
   ],
   "source": [
    "# Ridge Logistic Regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "\n",
    "logreg_ridge = LogisticRegression(solver=\"lbfgs\", random_state=0, penalty=\"l2\")\n",
    "logreg_ridge.fit(X, y)\n",
    "print(\"Accuracy of Ridge Logistic Regression on training set: {:.2f}\".format(logreg_ridge.score(X, y)))\n",
    "print(\"Accuracy of Ridge Logistic Regression on test set: {:.2f}\".format(logreg_ridge.score(X_test, y_test)))\n",
    "\n",
    "\n",
    "# Estimating Ridge Parameter\n",
    "\n",
    "best_c = 0\n",
    "best_score = 0\n",
    "\n",
    "for i in range(1,101):\n",
    "    c = (i/100)\n",
    "    score_list = list()\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        temp_model = LogisticRegression(solver=\"lbfgs\", random_state=0, penalty=\"l2\", C=c)\n",
    "        temp_model.fit(X[train_index], y[train_index])\n",
    "        score_list.append(temp_model.score(X[test_index], y[test_index]))\n",
    "    \n",
    "    if np.mean(score_list)>best_score:\n",
    "        best_score=np.mean(score_list)\n",
    "        best_c=c\n",
    "\n",
    "print(best_c)\n",
    "\n",
    "# Generalized Error\n",
    "\n",
    "gen_error_lrr = list()\n",
    "f1_lrr = list()\n",
    "ra_lrr = list()\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    logreg_ridge_gen = LogisticRegression(solver=\"lbfgs\", random_state=0, penalty=\"l2\", C=best_c)\n",
    "    logreg_ridge_gen.fit(X[train_index], y[train_index])\n",
    "    gen_error_lrr.append(logreg_ridge_gen.score(X[test_index], y[test_index]))\n",
    "    y_pred = logreg_ridge_gen.predict(X[test_index])\n",
    "    f1_lrr.append(f1_score(y[test_index],y_pred))\n",
    "    ra_lrr.append(roc_auc_score(y[test_index],y_pred))\n",
    "\n",
    "print(np.mean(gen_error_lrr))\n",
    "print(np.mean(f1_lrr))\n",
    "print(np.mean(ra_lrr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Lasso Logistic Regression on training set: 0.80\n",
      "Accuracy of Lasso Logistic Regression on test set: 0.81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jlund\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\jlund\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\jlund\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\jlund\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\jlund\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\jlund\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\jlund\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\jlund\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\jlund\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\jlund\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\jlund\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\jlund\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\jlund\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\jlund\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\jlund\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\jlund\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\jlund\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07\n",
      "0.8010254084627692\n",
      "0.5723260400530068\n",
      "0.7065065819075745\n"
     ]
    }
   ],
   "source": [
    "# Lasso Logistic Regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "\n",
    "logreg_lasso = LogisticRegression(solver='saga', random_state=0, penalty=\"l1\")\n",
    "logreg_lasso.fit(X, y)\n",
    "print(\"Accuracy of Lasso Logistic Regression on training set: {:.2f}\".format(logreg_lasso.score(X, y)))\n",
    "print(\"Accuracy of Lasso Logistic Regression on test set: {:.2f}\".format(logreg_lasso.score(X_test, y_test)))\n",
    "\n",
    "# Estimating Lasso Parameter\n",
    "\n",
    "best_c = 0\n",
    "best_score = 0\n",
    "\n",
    "for i in range(1,101):\n",
    "    c = (i/100)\n",
    "    score_list = list()\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        temp_model = LogisticRegression(solver=\"saga\", random_state=0, penalty=\"l1\", C=c)\n",
    "        temp_model.fit(X[train_index], y[train_index])\n",
    "        score_list.append(temp_model.score(X[test_index], y[test_index]))\n",
    "    \n",
    "    if np.mean(score_list)>best_score:\n",
    "        best_score=np.mean(score_list)\n",
    "        best_c=c\n",
    "\n",
    "print(best_c)\n",
    "\n",
    "# Generalized Error\n",
    "\n",
    "gen_error_lrl = list()\n",
    "f1_lrl = list()\n",
    "ra_lrl = list()\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    logreg_lasso_gen = LogisticRegression(solver=\"saga\", random_state=0, penalty=\"l2\", C=best_c)\n",
    "    logreg_lasso_gen.fit(X[train_index], y[train_index])\n",
    "    gen_error_lrl.append(logreg_lasso_gen.score(X[test_index], y[test_index]))\n",
    "    y_pred = logreg_lasso_gen.predict(X[test_index])\n",
    "    f1_lrl.append(f1_score(y[test_index],y_pred))\n",
    "    ra_lrl.append(roc_auc_score(y[test_index],y_pred))\n",
    "    \n",
    "print(np.mean(gen_error_lrl))\n",
    "print(np.mean(f1_lrl))\n",
    "print(np.mean(ra_lrl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminant analysis\n",
    "Assumes multivariate Gaussian distributions, while we have a majority of binary variables. Skip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of LDA on training set: 0.80\n",
      "Accuracy of LDA on test set: 0.81\n",
      "Generalized Error on LDA:  0.7988958593149669\n",
      "F1 score on LDA:  0.5710837633952383\n",
      "ROC-AUC Score on LDA:  0.7060737173292818\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X, y)\n",
    "print(\"Accuracy of LDA on training set: {:.2f}\".format(lda.score(X, y)))\n",
    "print(\"Accuracy of LDA on test set: {:.2f}\".format(lda.score(X_test, y_test)))\n",
    "\n",
    "# Generalized Error\n",
    "\n",
    "gen_error_lda = list()\n",
    "f1_lda = list()\n",
    "ra_lda = list()\n",
    "\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    lda_gen = LinearDiscriminantAnalysis()\n",
    "    lda_gen.fit(X[train_index], y[train_index])\n",
    "    gen_error_lda.append(lda_gen.score(X[test_index], y[test_index]))\n",
    "    y_pred = lda_gen.predict(X[test_index])\n",
    "    f1_lda.append(f1_score(y[test_index],y_pred))\n",
    "    ra_lda.append(roc_auc_score(y[test_index],y_pred))\n",
    "\n",
    "print(\"Generalized Error on LDA: \",np.mean(gen_error_lda))\n",
    "print(\"F1 score on LDA: \",np.mean(f1_lda))\n",
    "print(\"ROC-AUC Score on LDA: \",np.mean(ra_lda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of QDA on traning set: 0.76\n",
      "Accuracy of QDA on test set: 0.76\n",
      "Generalized Error on QDA:  0.7511510650895028\n",
      "F1 Score on QDA:  0.6162503370118168\n",
      "ROC-AUC Score on QDA:  0.7527692176254981\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "qda = QuadraticDiscriminantAnalysis()\n",
    "qda.fit(X, y)\n",
    "print(\"Accuracy of QDA on traning set: {:.2f}\".format(qda.score(X, y)))\n",
    "print(\"Accuracy of QDA on test set: {:.2f}\".format(qda.score(X_test, y_test)))\n",
    "\n",
    "# Generalized Error\n",
    "\n",
    "gen_error_qda = list()\n",
    "f1_qda = list()\n",
    "ra_qda = list()\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    qda_gen = QuadraticDiscriminantAnalysis()\n",
    "    qda_gen.fit(X[train_index], y[train_index])\n",
    "    gen_error_qda.append(qda_gen.score(X[test_index], y[test_index]))\n",
    "    y_pred = qda_gen.predict(X[test_index])\n",
    "    f1_qda.append(f1_score(y[test_index],y_pred))\n",
    "    ra_qda.append(roc_auc_score(y[test_index],y_pred))\n",
    "    \n",
    "print(\"Generalized Error on QDA: \",np.mean(gen_error_qda))\n",
    "print(\"F1 Score on QDA: \",np.mean(f1_qda))\n",
    "print(\"ROC-AUC Score on QDA: \",np.mean(ra_qda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.76\n",
      "Generalized Error on QDA:  0.7763504150762757\n",
      "F1 Score on QDA:  0.6096569994237091\n",
      "ROC-AUC Score on QDA:  0.739448361846373\n"
     ]
    }
   ],
   "source": [
    "# Estimating QDA regularization Parameter\n",
    "\n",
    "best_c = 0\n",
    "best_score = 0\n",
    "\n",
    "for i in range(1,101):\n",
    "    c = (i/100)\n",
    "    score_list = list()\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        temp_model = QuadraticDiscriminantAnalysis(reg_param=c)\n",
    "        temp_model.fit(X[train_index], y[train_index])\n",
    "        score_list.append(temp_model.score(X[test_index], y[test_index]))\n",
    "    \n",
    "    if np.mean(score_list)>best_score:\n",
    "        best_score=np.mean(score_list)\n",
    "        best_c=c\n",
    "\n",
    "print(best_c)\n",
    "\n",
    "# Generalized Error\n",
    "\n",
    "gen_error_qda = list()\n",
    "f1_qda = list()\n",
    "ra_qda = list()\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    qda_gen = QuadraticDiscriminantAnalysis(reg_param=best_c)\n",
    "    qda_gen.fit(X[train_index], y[train_index])\n",
    "    gen_error_qda.append(qda_gen.score(X[test_index], y[test_index]))\n",
    "    y_pred = qda_gen.predict(X[test_index])\n",
    "    f1_qda.append(f1_score(y[test_index],y_pred))\n",
    "    ra_qda.append(roc_auc_score(y[test_index],y_pred))\n",
    "    \n",
    "print(\"Generalized Error on QDA: \",np.mean(gen_error_qda))\n",
    "print(\"F1 Score on QDA: \",np.mean(f1_qda))\n",
    "print(\"ROC-AUC Score on QDA: \",np.mean(ra_qda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Decision Tree classifier on training set: 1.00\n",
      "Accuracy of Decision Tree classifier on test set: 0.71\n",
      "Generalized Error on Decision Tree:  0.7216844286560095\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "dtc = tree.DecisionTreeClassifier().fit(X, y)\n",
    "\n",
    "print(\"Accuracy of Decision Tree classifier on training set: {:.2f}\".format(dtc.score(X, y)))\n",
    "print(\"Accuracy of Decision Tree classifier on test set: {:.2f}\".format(dtc.score(X_test, y_test)))\n",
    "\n",
    "# Generalized Error\n",
    "gen_error_dt = list()\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    dt_gen = tree.DecisionTreeClassifier()\n",
    "    dt_gen.fit(X[train_index], y[train_index])\n",
    "    gen_error_dt.append(dt_gen.score(X[test_index], y[test_index]))\n",
    "    \n",
    "print(\"Generalized Error on Decision Tree: \",np.mean(gen_error_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import graphviz\n",
    "# viz = tree.export_graphviz(dtc, out_file=None, \n",
    "#                            feature_names=cols)\n",
    "# graph = graphviz.Source(viz)\n",
    "# graph.render(\"viz3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of KNN classifier on training set: 0.83\n",
      "Accuracy of KNN classifier on test set: 0.77\n",
      "Generalized Error on K Nearest Neighbour:  0.7532802993084162\n",
      "F1 Score on K Nearest Neighbour:  0.5183696406433148\n",
      "ROC-AUC Score on K Nearest Neighbour:  0.673015287041028\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier().fit(X, y)\n",
    "\n",
    "print(\"Accuracy of KNN classifier on training set: {:.2f}\".format(knn.score(X, y)))\n",
    "print(\"Accuracy of KNN classifier on test set: {:.2f}\".format(knn.score(X_test, y_test)))\n",
    "\n",
    "\n",
    "# Generalized Error\n",
    "gen_error_knn = list()\n",
    "f1_knn = list()\n",
    "ra_knn = list()\n",
    "\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    knn_gen = KNeighborsClassifier()\n",
    "    knn_gen.fit(X[train_index], y[train_index])\n",
    "    gen_error_knn.append(knn_gen.score(X[test_index], y[test_index]))\n",
    "    y_pred = knn_gen.predict(X[test_index])\n",
    "    f1_knn.append(f1_score(y[test_index],y_pred))\n",
    "    ra_knn.append(roc_auc_score(y[test_index],y_pred))\n",
    "    \n",
    "print(\"Generalized Error on K Nearest Neighbour: \",np.mean(gen_error_knn))\n",
    "print(\"F1 Score on K Nearest Neighbour: \",np.mean(f1_knn))\n",
    "print(\"ROC-AUC Score on K Nearest Neighbour: \",np.mean(ra_knn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of GNB classifier on training set: 0.76\n",
      "Accuracy of GNB classifier on test set: 0.78\n",
      "Generalized Error on Gaussian Naive Bayes:  0.7547015733847297\n",
      "F1 Score on Gaussian Naive Bayes:  0.6008011378571447\n",
      "ROC-AUC Score on Gaussian Naive Bayes:  0.7367622219679558\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB().fit(X, y)\n",
    "\n",
    "print(\"Accuracy of GNB classifier on training set: {:.2f}\".format(gnb.score(X, y)))\n",
    "print(\"Accuracy of GNB classifier on test set: {:.2f}\".format(gnb.score(X_test, y_test)))\n",
    "\n",
    "\n",
    "# Generalized Error\n",
    "\n",
    "gen_error_gnb = list()\n",
    "f1_gnb = list()\n",
    "ra_gnb = list()\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    gnb_gen = GaussianNB()\n",
    "    gnb_gen.fit(X[train_index], y[train_index])\n",
    "    gen_error_gnb.append(gnb_gen.score(X[test_index], y[test_index]))\n",
    "    y_pred = gnb_gen.predict(X[test_index])\n",
    "    f1_gnb.append(f1_score(y[test_index],y_pred))\n",
    "    ra_gnb.append(roc_auc_score(y[test_index],y_pred))\n",
    "    \n",
    "print(\"Generalized Error on Gaussian Naive Bayes: \",np.mean(gen_error_gnb))\n",
    "print(\"F1 Score on Gaussian Naive Bayes: \",np.mean(f1_gnb))\n",
    "print(\"ROC-AUC Score on Gaussian Naive Bayes: \",np.mean(ra_gnb))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of SVM classifier on training set: 0.82\n",
      "Accuracy of SVM classifier on test set: 0.81\n",
      "Generalized Error on Support Vector Machine:  0.7930394416940654\n",
      "F1 Score on Support Vector Machine:  0.5481000836959161\n",
      "ROC-AUC Score on Support Vector Machine:  0.6912129842434613\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC \n",
    "\n",
    "svm = SVC().fit(X, y)\n",
    "\n",
    "print(\"Accuracy of SVM classifier on training set: {:.2f}\".format(svm.score(X, y)))\n",
    "print(\"Accuracy of SVM classifier on test set: {:.2f}\".format(svm.score(X_test, y_test)))\n",
    "\n",
    "\n",
    "# Generalized Error\n",
    "\n",
    "gen_error_svm = list()\n",
    "f1_svm = list()\n",
    "ra_svm = list()\n",
    "\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    svm_gen = SVC()\n",
    "    svm_gen.fit(X[train_index], y[train_index])\n",
    "    gen_error_svm.append(svm_gen.score(X[test_index], y[test_index]))\n",
    "    y_pred = svm_gen.predict(X[test_index])\n",
    "    f1_svm.append(f1_score(y[test_index],y_pred))\n",
    "    ra_svm.append(roc_auc_score(y[test_index],y_pred))\n",
    "    \n",
    "print(\"Generalized Error on Support Vector Machine: \",np.mean(gen_error_svm))\n",
    "print(\"F1 Score on Support Vector Machine: \",np.mean(f1_svm))\n",
    "print(\"ROC-AUC Score on Support Vector Machine: \",np.mean(ra_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RBF SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "# from sklearn.gaussian_process.kernels import RBF\n",
    "\n",
    "# rbf = GaussianProcessClassifier(1.0 * RBF(1.0))\n",
    "# rbf.fit(X, y)\n",
    "\n",
    "# print(\"Accuracy of RBF SVM classifier on training set: {:.2f}\".format(rbf.score(X, y)))\n",
    "# print(\"Accuracy of RBF SVM classifier on test set: {:.2f}\".format(rbf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Random Forests classifier on training set: 0.98\n",
      "Accuracy of Random Forests classifier on test set: 0.79\n",
      "Generalized Error on Random Forests:  0.7790143985488077\n",
      "F1 Score on Random Forests:  0.6088578200873603\n",
      "ROC AUC Score on Random Forests:  0.7278069098782856\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X, y)\n",
    "\n",
    "print(\"Accuracy of Random Forests classifier on training set: {:.2f}\".format(rfc.score(X, y)))\n",
    "print(\"Accuracy of Random Forests classifier on test set: {:.2f}\".format(rfc.score(X_test, y_test)))\n",
    "\n",
    "# Generalized Error\n",
    "gen_error_rf = list()\n",
    "f1_rf = list()\n",
    "ra_rf = list()\n",
    "\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    rf_gen = RandomForestClassifier()\n",
    "    rf_gen.fit(X[train_index], y[train_index])\n",
    "    gen_error_rf.append(rf_gen.score(X[test_index], y[test_index]))\n",
    "    y_pred = svm_gen.predict(X[test_index])\n",
    "    f1_rf.append(f1_score(y[test_index],y_pred))\n",
    "    ra_rf.append(roc_auc_score(y[test_index],y_pred))\n",
    "    \n",
    "print(\"Generalized Error on Random Forests: \",np.mean(gen_error_rf))\n",
    "print(\"F1 Score on Random Forests: \",np.mean(f1_rf))\n",
    "print(\"ROC AUC Score on Random Forests: \",np.mean(ra_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Hyperparameters \n",
    "n_estimators = [int(x) for x in np.linspace(start = 10, stop = 2000, num = 100)] # Number of trees in forest\n",
    "max_features = ['auto'] # Number of features considered at each split\n",
    "max_depth = [int(x) for x in np.linspace(1, 110, num = 100)] # Max levels in the tree\n",
    "min_samples_split = [2, 5, 10] # Min number of samples required to split a node, \n",
    "min_samples_leaf = [1, 2, 3] # Min number of samples required at each leaf node\n",
    "bootstrap = [True, False] # Bootstrapped sample selection\n",
    "\n",
    "random_grid = {'n_estimators':n_estimators, \n",
    "               'max_features':max_features,\n",
    "               'max_depth':max_depth,\n",
    "               'min_samples_split':min_samples_split,\n",
    "               'min_samples_leaf':min_samples_leaf,\n",
    "               'bootstrap':bootstrap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:  8.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise-deprecating',\n",
       "          estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "          fit_params=None, iid='warn', n_iter=50, n_jobs=-1,\n",
       "          param_distributions={'n_estimators': [10, 30, 50, 70, 90, 110, 130, 150, 170, 190, 211, 231, 251, 271, 291, 311, 331, 351, 371, 391, 412, 432, 452, 472, 492, 512, 532, 552, 572, 592, 613, 633, 653, 673, 693, 713, 733, 753, 773, 793, 814, 834, 854, 874, 894, 914, 934, 954, 974, 994, 1015, 1035, 1055,...8, 110], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 3], 'bootstrap': [True, False]},\n",
       "          pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "          return_train_score='warn', scoring=None, verbose=2)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, \n",
    "                               n_iter = 50, cv = 3, verbose = 2, random_state = 42,\n",
    "                               n_jobs = -1)\n",
    "\n",
    "rf_random.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=67, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=3, min_samples_split=10,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=1738, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generalized Error on Random Forests:  0.8006679641736897\n"
     ]
    }
   ],
   "source": [
    "# Generalized Error\n",
    "gen_error_rf = list()\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    rf_best = rf_random.best_estimator_\n",
    "    rf_best.fit(X[train_index], y[train_index])\n",
    "    gen_error_rf.append(rf_best.score(X[test_index], y[test_index]))\n",
    "    \n",
    "print(\"Generalized Error on Random Forests: \",np.mean(gen_error_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8062455642299503"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_best.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6949561108408294"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "rf_y_pred = rf_best.predict(X_test)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, rf_y_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "importances = list(rf_best.feature_importances_)\n",
    "x_values = list(range(len(importances)))\n",
    "plt.bar(x_values, importances)\n",
    "features_list = df.columns.values[1:-1]\n",
    "plt.xticks(x_values, features_list, rotation = 'vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of AdaBoost classifier on training set: 0.80\n",
      "Accuracy of AdaBoost classifier on test set: 0.81\n",
      "Generalized Error on AdaBoost:  0.7964094957358628\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada = AdaBoostClassifier()\n",
    "ada.fit(X, y)\n",
    "\n",
    "print(\"Accuracy of AdaBoost classifier on training set: {:.2f}\".format(ada.score(X, y)))\n",
    "print(\"Accuracy of AdaBoost classifier on test set: {:.2f}\".format(ada.score(X_test, y_test)))\n",
    "####\n",
    "\n",
    "# Generalized Error\n",
    "\n",
    "gen_error_ab = list()\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    ab_gen = AdaBoostClassifier()\n",
    "    ab_gen.fit(X[train_index], y[train_index])\n",
    "    gen_error_ab.append(ab_gen.score(X[test_index], y[test_index]))\n",
    "    \n",
    "print(\"Generalized Error on AdaBoost: \",np.mean(gen_error_ab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of MLP classifier on training set: 0.81\n",
      "Accuracy of MLP classifier on test set: 0.82\n",
      "Generalized Error on NN:  0.7972994847763374\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(alpha=1)\n",
    "mlp.fit(X, y)\n",
    "\n",
    "print(\"Accuracy of MLP classifier on training set: {:.2f}\".format(mlp.score(X, y)))\n",
    "print(\"Accuracy of MLP classifier on test set: {:.2f}\".format(mlp.score(X_test, y_test)))\n",
    "\n",
    "\n",
    "# Generalized Error\n",
    "\n",
    "gen_error_nn = list()\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    nn_gen = MLPClassifier(alpha=1)\n",
    "    nn_gen.fit(X[train_index], y[train_index])\n",
    "    gen_error_nn.append(nn_gen.score(X[test_index], y[test_index]))\n",
    "    \n",
    "print(\"Generalized Error on NN: \",np.mean(gen_error_nn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Hyperparameter of Neural Net to Maximise Generalized Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akshaysundar/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/akshaysundar/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/akshaysundar/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/akshaysundar/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/akshaysundar/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/akshaysundar/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/akshaysundar/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/akshaysundar/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/akshaysundar/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/akshaysundar/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/akshaysundar/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/akshaysundar/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.7983642593502387\n"
     ]
    }
   ],
   "source": [
    "# Estimating Hyper-Parameter\n",
    "\n",
    "best_alpha = 0\n",
    "best_score = 0\n",
    "\n",
    "for i in range(1,6):\n",
    "    c = (i/5)\n",
    "    score_list = list()\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        temp_model = MLPClassifier(alpha=c)\n",
    "        temp_model.fit(X[train_index], y[train_index])\n",
    "        score_list.append(temp_model.score(X[test_index], y[test_index]))\n",
    "    \n",
    "    if np.mean(score_list)>best_score:\n",
    "        best_score=np.mean(score_list)\n",
    "        best_alpha=c\n",
    "\n",
    "print(best_alpha)\n",
    "print(best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generalized Error on MLP with tuned Hyperparameter:  0.7990731642795057\n",
      "F1 Score on MLP with tuned Hyperparameter:  0.5734963523285469\n",
      "ROC-AUC Score on MLP with tuned Hyperparameter:  0.7071452827915458\n"
     ]
    }
   ],
   "source": [
    "# Generalized Error\n",
    "\n",
    "gen_error_nn = list()\n",
    "f1_nn = list()\n",
    "ra_nn = list()\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    nn_gen = MLPClassifier(alpha=best_alpha)\n",
    "    nn_gen.fit(X[train_index], y[train_index])\n",
    "    gen_error_nn.append(nn_gen.score(X[test_index], y[test_index]))\n",
    "    y_pred = nn_gen.predict(X[test_index])\n",
    "    f1_nn.append(f1_score(y[test_index],y_pred))\n",
    "    ra_nn.append(roc_auc_score(y[test_index],y_pred))\n",
    "    \n",
    "print(\"Generalized Error on MLP with tuned Hyperparameter: \",np.mean(gen_error_nn))\n",
    "print(\"F1 Score on MLP with tuned Hyperparameter: \",np.mean(f1_nn))\n",
    "print(\"ROC-AUC Score on MLP with tuned Hyperparameter: \",np.mean(ra_nn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Model Performances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:  0.7994290339241401\n",
      "Ridge Regression:  0.8012030283561972\n",
      "Lasso Regression:  0.7981860095990326\n",
      "LDA:  0.795524545557613\n",
      "QDA:  0.7763504150762757\n",
      "Decision Tree:  0.7216844286560095\n",
      "K Nearest Neighbour:  0.7532802993084162\n",
      "Gaussian Naive Bayes:  0.7547015733847297\n",
      "SVM:  0.7930394416940654\n",
      "Random Forests:  0.7790143985488077\n",
      "MLP:  0.7990731642795057\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic Regression: \",np.mean(gen_error_lrb))\n",
    "print(\"Ridge Regression: \",np.mean(gen_error_lrr))\n",
    "print(\"Lasso Regression: \",np.mean(gen_error_lrl))\n",
    "print(\"LDA: \",np.mean(gen_error_lda))\n",
    "print(\"QDA: \",np.mean(gen_error_qda))\n",
    "print(\"Decision Tree: \",np.mean(gen_error_dt))\n",
    "print(\"K Nearest Neighbour: \",np.mean(gen_error_knn))\n",
    "print(\"Gaussian Naive Bayes: \",np.mean(gen_error_gnb))\n",
    "print(\"SVM: \",np.mean(gen_error_svm))\n",
    "print(\"Random Forests: \",np.mean(gen_error_rf))\n",
    "print(\"MLP: \",np.mean(gen_error_nn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:  0.5712763973228642\n",
      "Ridge Regression:  0.5732844092701364\n",
      "Lasso Regression:  0.5792664970840298\n",
      "LDA:  0.5843800363403059\n",
      "QDA:  0.6096569994237091\n",
      "K Nearest Neighbour:  0.5183696406433148\n",
      "Gaussian Naive Bayes:  0.6008011378571447\n",
      "SVM:  0.5481000836959161\n",
      "Random Forests:  0.6088578200873603\n",
      "MLP:  0.5734963523285469\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic Regression: \",np.mean(f1_lrb))\n",
    "print(\"Ridge Regression: \",np.mean(f1_lrr))\n",
    "print(\"Lasso Regression: \",np.mean(f1_lrl))\n",
    "print(\"LDA: \",np.mean(f1_lda))\n",
    "print(\"QDA: \",np.mean(f1_qda))\n",
    "print(\"K Nearest Neighbour: \",np.mean(f1_knn))\n",
    "print(\"Gaussian Naive Bayes: \",np.mean(f1_gnb))\n",
    "print(\"SVM: \",np.mean(f1_svm))\n",
    "print(\"Random Forests: \",np.mean(f1_rf))\n",
    "print(\"MLP: \",np.mean(f1_nn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC AUC Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:  0.7059542035844095\n",
      "Ridge Regression:  0.7070690956857325\n",
      "Lasso Regression:  0.7110574075076286\n",
      "LDA:  0.7152567363535336\n",
      "QDA:  0.739448361846373\n",
      "K Nearest Neighbour:  0.673015287041028\n",
      "Gaussian Naive Bayes:  0.7367622219679558\n",
      "SVM:  0.6912129842434613\n",
      "Random Forests:  0.7278069098782856\n",
      "MLP:  0.7071452827915458\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic Regression: \",np.mean(ra_lrb))\n",
    "print(\"Ridge Regression: \",np.mean(ra_lrr))\n",
    "print(\"Lasso Regression: \",np.mean(ra_lrl))\n",
    "print(\"LDA: \",np.mean(ra_lda))\n",
    "print(\"QDA: \",np.mean(ra_qda))\n",
    "print(\"K Nearest Neighbour: \",np.mean(ra_knn))\n",
    "print(\"Gaussian Naive Bayes: \",np.mean(ra_gnb))\n",
    "print(\"SVM: \",np.mean(ra_svm))\n",
    "print(\"Random Forests: \",np.mean(ra_rf))\n",
    "print(\"MLP: \",np.mean(ra_nn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
