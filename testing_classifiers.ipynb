{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"telco_train.csv\", index_col=0)\n",
    "df_test = pd.read_csv(\"telco_test.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop([\"customer_id\", \"churn\"], axis=1)\n",
    "#y = pd.reset_index(df.churn)\n",
    "y = df.churn.reset_index().churn\n",
    "\n",
    "X_test = df_test.drop([\"customer_id\", \"churn\"], axis=1)\n",
    "y_test = df_test.churn.reset_index().churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.astype(np.float64)\n",
    "X_test = X_test.astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=10, random_state=None, shuffle=False)\n"
     ]
    }
   ],
   "source": [
    "#Split for Cross Validation\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=10)\n",
    "kf.get_n_splits(X)\n",
    "print(kf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic Regression on training set: 0.80\n",
      "Accuracy of Logistic Regression on test set: 0.82\n",
      "0.797651575274303\n"
     ]
    }
   ],
   "source": [
    "# Simple Logistic Regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "\n",
    "logreg_basic = LogisticRegression(solver=\"lbfgs\", random_state=0)\n",
    "logreg_basic.fit(X, y)\n",
    "print(\"Accuracy of Logistic Regression on training set: {:.2f}\".format(logreg_basic.score(X, y)))\n",
    "print(\"Accuracy of Logistic Regression on test set: {:.2f}\".format(logreg_basic.score(X_test, y_test)))\n",
    "\n",
    "# Generalized Error\n",
    "\n",
    "gen_error_lrb = list()\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    logreg_basic_gen = LogisticRegression(solver=\"lbfgs\", random_state=0)\n",
    "    logreg_basic_gen.fit(X[train_index], y[train_index])\n",
    "    gen_error_lrb.append(logreg_basic_gen.score(X[test_index], y[test_index]))\n",
    "    \n",
    "print(np.mean(gen_error_lrb))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Ridge Logistic Regression on training set: 0.80\n",
      "Accuracy of Ridge Logistic Regression on test set: 0.82\n",
      "0.03\n",
      "0.7996047642442337\n"
     ]
    }
   ],
   "source": [
    "# Ridge Logistic Regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "\n",
    "logreg_ridge = LogisticRegression(solver=\"lbfgs\", random_state=0, penalty=\"l2\")\n",
    "logreg_ridge.fit(X, y)\n",
    "print(\"Accuracy of Ridge Logistic Regression on training set: {:.2f}\".format(logreg_ridge.score(X, y)))\n",
    "print(\"Accuracy of Ridge Logistic Regression on test set: {:.2f}\".format(logreg_ridge.score(X_test, y_test)))\n",
    "\n",
    "\n",
    "# Estimating Ridge Parameter\n",
    "\n",
    "best_c = 0\n",
    "best_score = 0\n",
    "\n",
    "for i in range(1,101):\n",
    "    c = (i/100)\n",
    "    score_list = list()\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        temp_model = LogisticRegression(solver=\"lbfgs\", random_state=0, penalty=\"l2\", C=c)\n",
    "        temp_model.fit(X[train_index], y[train_index])\n",
    "        score_list.append(temp_model.score(X[test_index], y[test_index]))\n",
    "    \n",
    "    if np.mean(score_list)>best_score:\n",
    "        best_score=np.mean(score_list)\n",
    "        best_c=c\n",
    "\n",
    "print(best_c)\n",
    "\n",
    "# Generalized Error\n",
    "\n",
    "gen_error_lrr = list()\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    logreg_ridge_gen = LogisticRegression(solver=\"lbfgs\", random_state=0, penalty=\"l2\", C=best_c)\n",
    "    logreg_ridge_gen.fit(X[train_index], y[train_index])\n",
    "    gen_error_lrr.append(logreg_ridge_gen.score(X[test_index], y[test_index]))\n",
    "    \n",
    "print(np.mean(gen_error_lrr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Lasso Logistic Regression on training set: 0.80\n",
      "Accuracy of Lasso Logistic Regression on test set: 0.82\n",
      "0.06\n",
      "0.7987172946348714\n"
     ]
    }
   ],
   "source": [
    "# Lasso Logistic Regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "\n",
    "logreg_lasso = LogisticRegression(solver='saga', random_state=0, penalty=\"l1\")\n",
    "logreg_lasso.fit(X, y)\n",
    "print(\"Accuracy of Lasso Logistic Regression on training set: {:.2f}\".format(logreg_lasso.score(X, y)))\n",
    "print(\"Accuracy of Lasso Logistic Regression on test set: {:.2f}\".format(logreg_lasso.score(X_test, y_test)))\n",
    "\n",
    "# Estimating Lasso Parameter\n",
    "\n",
    "best_c = 0\n",
    "best_score = 0\n",
    "\n",
    "for i in range(1,101):\n",
    "    c = (i/100)\n",
    "    score_list = list()\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        temp_model = LogisticRegression(solver=\"saga\", random_state=0, penalty=\"l1\", C=c)\n",
    "        temp_model.fit(X[train_index], y[train_index])\n",
    "        score_list.append(temp_model.score(X[test_index], y[test_index]))\n",
    "    \n",
    "    if np.mean(score_list)>best_score:\n",
    "        best_score=np.mean(score_list)\n",
    "        best_c=c\n",
    "\n",
    "print(best_c)\n",
    "\n",
    "# Generalized Error\n",
    "\n",
    "gen_error_lrl = list()\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    logreg_lasso_gen = LogisticRegression(solver=\"saga\", random_state=0, penalty=\"l2\", C=best_c)\n",
    "    logreg_lasso_gen.fit(X[train_index], y[train_index])\n",
    "    gen_error_lrl.append(logreg_lasso_gen.score(X[test_index], y[test_index]))\n",
    "    \n",
    "print(np.mean(gen_error_lrl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA, QDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of LDA on training set: 0.80\n",
      "Accuracy of LDA on test set: 0.82\n",
      "Generalized Error on LDA:  0.7941013819079652\n",
      "Accuracy of QDA on traning set: 0.77\n",
      "Accuracy of QDA on test set: 0.77\n",
      "Generalized Error on QDA:  0.7594884924983938\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "\n",
    "#LDA\n",
    "\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X, y)\n",
    "print(\"Accuracy of LDA on training set: {:.2f}\".format(lda.score(X, y)))\n",
    "print(\"Accuracy of LDA on test set: {:.2f}\".format(lda.score(X_test, y_test)))\n",
    "\n",
    "# Generalized Error\n",
    "\n",
    "gen_error_lda = list()\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    lda_gen = LinearDiscriminantAnalysis()\n",
    "    lda_gen.fit(X[train_index], y[train_index])\n",
    "    gen_error_lda.append(lda_gen.score(X[test_index], y[test_index]))\n",
    "    \n",
    "print(\"Generalized Error on LDA: \",np.mean(gen_error_lda))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#QDA\n",
    "\n",
    "\n",
    "qda = QuadraticDiscriminantAnalysis()\n",
    "qda.fit(X, y)\n",
    "print(\"Accuracy of QDA on traning set: {:.2f}\".format(qda.score(X, y)))\n",
    "print(\"Accuracy of QDA on test set: {:.2f}\".format(qda.score(X_test, y_test)))\n",
    "\n",
    "# Generalized Error\n",
    "\n",
    "gen_error_qda = list()\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    qda_gen = QuadraticDiscriminantAnalysis()\n",
    "    qda_gen.fit(X[train_index], y[train_index])\n",
    "    gen_error_qda.append(qda_gen.score(X[test_index], y[test_index]))\n",
    "    \n",
    "print(\"Generalized Error on QDA: \",np.mean(gen_error_qda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Decision Tree classifier on training set: 1.00\n",
      "Accuracy of Decision Tree classifier on test set: 0.73\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dtc = DecisionTreeClassifier().fit(X, y)\n",
    "\n",
    "print(\"Accuracy of Decision Tree classifier on training set: {:.2f}\".format(dtc.score(X, y)))\n",
    "print(\"Accuracy of Decision Tree classifier on test set: {:.2f}\".format(dtc.score(X_test, y_test)))\n",
    "\n",
    "\n",
    "\n",
    "# Generalized Error\n",
    "\n",
    "gen_error_dt = list()\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    dt_gen = DecisionTreeClassifier()\n",
    "    dt_gen.fit(X[train_index], y[train_index])\n",
    "    gen_error_dt.append(dt_gen.score(X[test_index], y[test_index]))\n",
    "    \n",
    "print(\"Generalized Error on Decision Tree: \",np.mean(gen_error_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of KNN classifier on training set: 0.83\n",
      "Accuracy of KNN classifier on test set: 0.77\n",
      "Generalized Error on K Nearest Neighbour:  0.7600207223209\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier().fit(X, y)\n",
    "\n",
    "print(\"Accuracy of KNN classifier on training set: {:.2f}\".format(knn.score(X, y)))\n",
    "print(\"Accuracy of KNN classifier on test set: {:.2f}\".format(knn.score(X_test, y_test)))\n",
    "\n",
    "\n",
    "# Generalized Error\n",
    "\n",
    "gen_error_knn = list()\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    knn_gen = KNeighborsClassifier()\n",
    "    knn_gen.fit(X[train_index], y[train_index])\n",
    "    gen_error_knn.append(knn_gen.score(X[test_index], y[test_index]))\n",
    "    \n",
    "print(\"Generalized Error on K Nearest Neighbour: \",np.mean(gen_error_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of GNB classifier on training set: 0.76\n",
      "Accuracy of GNB classifier on test set: 0.77\n",
      "Generalized Error on Gaussian Naive Bayes:  0.7568295478880869\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB().fit(X, y)\n",
    "\n",
    "print(\"Accuracy of GNB classifier on training set: {:.2f}\".format(gnb.score(X, y)))\n",
    "print(\"Accuracy of GNB classifier on test set: {:.2f}\".format(gnb.score(X_test, y_test)))\n",
    "\n",
    "\n",
    "# Generalized Error\n",
    "\n",
    "gen_error_gnb = list()\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    gnb_gen = GaussianNB()\n",
    "    gnb_gen.fit(X[train_index], y[train_index])\n",
    "    gen_error_gnb.append(gnb_gen.score(X[test_index], y[test_index]))\n",
    "    \n",
    "print(\"Generalized Error on Gaussian Naive Bayes: \",np.mean(gen_error_gnb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of SVM classifier on training set: 0.82\n",
      "Accuracy of SVM classifier on test set: 0.81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akshaysundar/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/akshaysundar/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/akshaysundar/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/akshaysundar/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/akshaysundar/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/akshaysundar/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/akshaysundar/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/akshaysundar/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/akshaysundar/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/akshaysundar/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generalized Error on Support Vector Machine:  0.7957015355932631\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC \n",
    "\n",
    "svm = SVC().fit(X, y)\n",
    "\n",
    "print(\"Accuracy of SVM classifier on training set: {:.2f}\".format(svm.score(X, y)))\n",
    "print(\"Accuracy of SVM classifier on test set: {:.2f}\".format(svm.score(X_test, y_test)))\n",
    "\n",
    "\n",
    "# Generalized Error\n",
    "\n",
    "gen_error_svm = list()\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    svm_gen = SVC()\n",
    "    svm_gen.fit(X[train_index], y[train_index])\n",
    "    gen_error_svm.append(svm_gen.score(X[test_index], y[test_index]))\n",
    "    \n",
    "print(\"Generalized Error on Support Vector Machine: \",np.mean(gen_error_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RBF SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from sklearn.gaussian_process import GaussianProcessClassifier\\nfrom sklearn.gaussian_process.kernels import RBF\\n\\nrbf = GaussianProcessClassifier(1.0 * RBF(1.0))\\nrbf.fit(X, y)\\n\\nprint(\"Accuracy of RBF SVM classifier on training set: {:.2f}\".format(rbf.score(X, y)))\\nprint(\"Accuracy of RBF SVM classifier on test set: {:.2f}\".format(rbf.score(X_test, y_test)))'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "# from sklearn.gaussian_process.kernels import RBF\n",
    "\n",
    "# rbf = GaussianProcessClassifier(1.0 * RBF(1.0))\n",
    "# rbf.fit(X, y)\n",
    "\n",
    "# print(\"Accuracy of RBF SVM classifier on training set: {:.2f}\".format(rbf.score(X, y)))\n",
    "# print(\"Accuracy of RBF SVM classifier on test set: {:.2f}\".format(rbf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Random Forests classifier on training set: 0.87\n",
      "Accuracy of Random Forests classifier on test set: 0.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akshaysundar/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/akshaysundar/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/akshaysundar/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/akshaysundar/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/akshaysundar/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/akshaysundar/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/akshaysundar/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/akshaysundar/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/akshaysundar/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generalized Error on Random Forests:  0.7777729488681456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akshaysundar/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(max_depth=10, n_estimators=10, max_features=1)\n",
    "rfc.fit(X, y)\n",
    "\n",
    "print(\"Accuracy of Random Forests classifier on training set: {:.2f}\".format(rfc.score(X, y)))\n",
    "print(\"Accuracy of Random Forests classifier on test set: {:.2f}\".format(rfc.score(X_test, y_test)))\n",
    "\n",
    "\n",
    "# Generalized Error\n",
    "\n",
    "gen_error_rf = list()\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    rf_gen = RandomForestClassifier()\n",
    "    rf_gen.fit(X[train_index], y[train_index])\n",
    "    gen_error_rf.append(rf_gen.score(X[test_index], y[test_index]))\n",
    "    \n",
    "print(\"Generalized Error on Random Forests: \",np.mean(gen_error_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of AdaBoost classifier on training set: 0.80\n",
      "Accuracy of AdaBoost classifier on test set: 0.81\n",
      "Generalized Error on AdaBoost:  0.7964094957358628\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada = AdaBoostClassifier()\n",
    "ada.fit(X, y)\n",
    "\n",
    "print(\"Accuracy of AdaBoost classifier on training set: {:.2f}\".format(ada.score(X, y)))\n",
    "print(\"Accuracy of AdaBoost classifier on test set: {:.2f}\".format(ada.score(X_test, y_test)))\n",
    "\n",
    "\n",
    "# Generalized Error\n",
    "\n",
    "gen_error_ab = list()\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    ab_gen = AdaBoostClassifier()\n",
    "    ab_gen.fit(X[train_index], y[train_index])\n",
    "    gen_error_ab.append(ab_gen.score(X[test_index], y[test_index]))\n",
    "    \n",
    "print(\"Generalized Error on AdaBoost: \",np.mean(gen_error_ab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of MLP classifier on training set: 0.81\n",
      "Accuracy of MLP classifier on test set: 0.82\n",
      "Generalized Error on AdaBoost:  0.7997817542798835\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(alpha=1)\n",
    "mlp.fit(X, y)\n",
    "\n",
    "print(\"Accuracy of MLP classifier on training set: {:.2f}\".format(mlp.score(X, y)))\n",
    "print(\"Accuracy of MLP classifier on test set: {:.2f}\".format(mlp.score(X_test, y_test)))\n",
    "\n",
    "\n",
    "# Generalized Error\n",
    "\n",
    "gen_error_nn = list()\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    nn_gen = MLPClassifier(alpha=1)\n",
    "    nn_gen.fit(X[train_index], y[train_index])\n",
    "    gen_error_nn.append(nn_gen.score(X[test_index], y[test_index]))\n",
    "    \n",
    "print(\"Generalized Error on AdaBoost: \",np.mean(gen_error_nn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Hyperparameter of Neural Net to Maximise Generalized Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akshaysundar/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/akshaysundar/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/akshaysundar/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/akshaysundar/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/akshaysundar/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/akshaysundar/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/akshaysundar/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/akshaysundar/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/Users/akshaysundar/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6\n",
      "0.7992495244573775\n"
     ]
    }
   ],
   "source": [
    "# Estimating Hyper-Parameter\n",
    "\n",
    "best_alpha = 0\n",
    "best_score = 0\n",
    "\n",
    "for i in range(1,6):\n",
    "    c = (i/5)\n",
    "    score_list = list()\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        temp_model = MLPClassifier(alpha=c)\n",
    "        temp_model.fit(X[train_index], y[train_index])\n",
    "        score_list.append(temp_model.score(X[test_index], y[test_index]))\n",
    "    \n",
    "    if np.mean(score_list)>best_score:\n",
    "        best_score=np.mean(score_list)\n",
    "        best_alpha=c\n",
    "\n",
    "print(best_alpha)\n",
    "print(best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generalized Error on MLP with tuned Hyperparameter:  0.79641044052253\n"
     ]
    }
   ],
   "source": [
    "# Generalized Error\n",
    "\n",
    "gen_error_nn = list()\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    nn_gen = MLPClassifier(alpha=0.6)\n",
    "    nn_gen.fit(X[train_index], y[train_index])\n",
    "    gen_error_nn.append(nn_gen.score(X[test_index], y[test_index]))\n",
    "    \n",
    "print(\"Generalized Error on MLP with tuned Hyperparameter: \",np.mean(gen_error_nn))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
